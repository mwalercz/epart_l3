\documentclass[a4paper, 11pt, wide]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage[MeX]{polski}
\usepackage{float}
\title{EPART laboratory 3, 4 report}
\author{Maciej Walerczuk}
\date{09.05.2018}
\begin{document}
  \maketitle
  \section{Description of canonical voting method}
  \section{Error rates of individual basic classifiers}
  \begin{table}[H]
\begin{center}
{\tiny
\begin{tabular}{ |c|c|c| } 
 \hline
 positive class & negative class & error rate \\ 
 \hline
    1 & 2 & 0.0022898 \\ 
    7 & 10 & 0.0037078 \\ 
    7 & 8 & 0.0038578 \\
    2 & 7 & 0.0041074 \\
    1 & 8 & 0.0048408 \\
    4 & 5 & 0.0050948 \\
    1 & 5 & 0.0053549 \\
    2 & 10 & 0.0061461 \\
    4 & 7 & 0.0065566 \\
    2 & 5 & 0.0065957 \\
    1 & 10 & 0.0068228 \\
    2 & 6 & 0.0070706 \\
    1 & 4 & 0.0076323 \\
    2 & 8 & 0.0080726 \\
    5 & 9 & 0.0085521 \\
    6 & 8 & 0.0091563 \\
    5 & 7 & 0.0094388 \\
    1 & 7 & 0.0099654 \\
    2 & 4 & 0.010021 \\
    1 & 9 & 0.010362 \\
    2 & 3 & 0.010472 \\
    5 & 6 & 0.010566 \\
    8 & 9 & 0.010977 \\
    1 & 3 & 0.011279 \\
    5 & 8 & 0.012224 \\
    4 & 8 & 0.012504 \\
    3 & 8 & 0.014726 \\
    7 & 9 & 0.014955 \\
    3 & 10 & 0.015033 \\
    6 & 10 & 0.015479 \\
    1 & 6 & 0.016132 \\
    9 & 10 & 0.01678 \\
    4 & 10 & 0.017798 \\
    3 & 5 & 0.018475 \\
    2 & 9 & 0.020329 \\
    3 & 7 & 0.020714 \\
    3 & 6 & 0.022761 \\
    6 & 7 & 0.022842 \\
    3 & 9 & 0.028538 \\
    3 & 4 & 0.029448 \\
    5 & 10 & 0.033246 \\
    4 & 9 & 0.03355 \\
    6 & 9 & 0.041519 \\
    8 & 10 & 0.042983 \\
    4 & 6 & 0.043109 \\
 \hline
\end{tabular}
}
\end{center}
\caption{Error rates for each classifier from canonical solution}
\end{table}
Each row in table consists of positive, negative class (to obtain digit 
each class needs to be decremented) and error rate. Error rate was calculated as
sum of false positives and false negatives divided by all samples (of those two 
digits which were classified by concrete classifer).
Rows are ordered by error rate value (ascending).
It's not a big surprise that the best classifier (when taking error rate into account) 
is the one that distinguishes between 0 and 1 (class 1 and 2) - it makes mistake only ~0.2\% times.
The worst classifier in this case is 3 and 5 (class 4 and 6) classifier, its error
rate is ~4.3\%, but it's followed closely by 7, 9 (4.2\%) and 5, 8 (4.1\%)
  \section{Classification quality and confusion matrices}
  \begin{table}[H]
  \begin{center}
  \begin{tabular}{ |c|c|c| } 
  \hline
  OK & ERROR & REJECTION \\ 
  \hline
   0.92273 & 0.0533 & 0.023967 \\
  \hline
  \end{tabular}
  \end{center}
  \caption{Training Set Canonical Classification Quality}
  \end{table}
  \begin{table}[H]
  \begin{center}
  \begin{tabular}{ |c|c|c| } 
  \hline
  OK & ERROR & REJECTION \\ 
  \hline
  0.9247 & 0.0534 & 0.0219 \\
  \hline
  \end{tabular}
  \caption{Test Set Canonical Classification Quality }
  \end{center}
  \end{table}
  Canonical solution givers fairly good results.
  In both test and training set quaility is similar, around 92\% samples are correctly
  classified. \\
  \begin{table}[H]
  \begin{center}
  \begin{tabular}{ |c|c c c c c c c c c c|c| } 
  \hline
   & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & R\\ 
  \hline
	 0 & 5696 & 0 & 23 & 9 & 8 & 52 & 33 & 3 & 17 & 2 & 80 \\
	 1 & 1 & 6494 & 35 & 16 & 11 & 15 & 4 & 16 & 68 & 5 & 77 \\
	 2 & 21 & 17 & 5447 & 56 & 53 & 22 & 54 & 33 & 69 & 13 & 173 \\
	 3 & 5 & 16 & 82 & 5522 & 0 & 184 & 15 & 28 & 69 & 29 & 181 \\
	 4 & 6 & 17 & 38 & 2 & 5461 & 5 & 27 & 20 & 19 & 148 & 99 \\
	 5 & 24 & 7 & 29 & 164 & 15 & 4781 & 48 & 7 & 92 & 23 & 231 \\
	 6 & 27 & 11 & 62 & 2 & 27 & 80 & 5596 & 0 & 25 & 0 & 88 \\
	 7 & 9 & 19 & 46 & 25 & 33 & 9 & 3 & 5853 & 11 & 116 & 141 \\
	 8 & 11 & 73 & 51 & 102 & 13 & 128 & 28 & 11 & 5186 & 32 & 216 \\
	 9 & 13 & 18 & 22 & 40 & 146 & 24 & 2 & 169 & 35 & 5328 & 152 \\
   \hline
  \end{tabular}
  \end{center}
  \caption{Training Set Confusion Matrix}
  \end{table}
  \begin{table}[H]
  \begin{center}
  \begin{tabular}{ |c|c c c c c c c c c c|c| } 
  \hline
   & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & R\\ 
  \hline
   0 & 948 & 0 & 3 & 2 & 0 & 5 & 10 & 1 & 1 & 0 & 10 \\
	 1 & 0 & 1099 & 2 & 2 & 0 & 4 & 1 & 2 & 13 & 0 & 12 \\
	 2 & 4 & 1 & 951 & 4 & 9 & 2 & 8 & 5 & 17 & 1 & 30 \\
	 3 & 0 & 0 & 8 & 933 & 0 & 21 & 0 & 7 & 13 & 3 & 25 \\
	 4 & 1 & 0 & 6 & 1 & 920 & 0 & 6 & 2 & 2 & 23 & 21 \\
	 5 & 5 & 2 & 5 & 33 & 2 & 788 & 6 & 1 & 20 & 5 & 25 \\
	 6 & 8 & 2 & 9 & 1 & 7 & 16 & 902 & 0 & 1 & 0 & 12 \\
	 7 & 1 & 4 & 22 & 9 & 3 & 0 & 0 & 944 & 3 & 17 & 25 \\
	 8 & 3 & 2 & 4 & 23 & 4 & 29 & 7 & 3 & 862 & 3 & 34 \\
	 9 & 5 & 5 & 2 & 8 & 28 & 4 & 0 & 27 & 5 & 900 & 25 \\
   \hline
  \end{tabular}
  \end{center}
  \caption{Test Set Confusion Matrix}
  \end{table}
  \section{Description of non standard two-digit classification}
  Classifier handling 3 and 5 digits (4, 6 classes) is the weakest classifier.
  Clustering was performed for both digits and then OVO classificators were
  trained among the clustered classes. Error rates for each of those classifiers 
  was calculated in the same way as explained in canonical solution section.
  The worst classifier (among clustered classes) has 2.5\% error rate, which is
  much better then 4.3\% from canonical solution. The best classifier has 0.07\% error rate.
  \begin{table}[H]
  \begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 positive class & negative class & error rate \\ 
 \hline
	 4 & 6 & 0.00073783  \\
	 4 & 6 & 0.0014053  \\
	 4 & 6 & 0.0072135  \\
	 4 & 6 & 0.0077478  \\
	 4 & 6 & 0.010339  \\
	 4 & 6 & 0.012325  \\
	 4 & 6 & 0.016467  \\
	 4 & 6 & 0.016688  \\
	 4 & 6 & 0.026301  \\
 \hline
\end{tabular}
\caption{Error rates for weakest classifier after clustering}
\end{center}
\end{table}
  \section{Final classification quality}
  \begin{table}[H]
  \begin{center}
  \begin{tabular}{ |c|c|c| } 
  \hline
  OK & ERROR & REJECTION \\ 
  \hline
  0.91188 & 0.055283 & 0.032833 \\
  \hline
  \end{tabular}
  \end{center}
  \caption{Training Set Final Classification Quality}
  \end{table}
  \begin{table}[H]
  \begin{center}
  \begin{tabular}{ |c|c|c| } 
  \hline
  OK & ERROR & REJECTION \\ 
  \hline
  0.9143 & 0.0539 & 0.0318 \\
  \hline
  \end{tabular}
  \caption{Test Set Final Classification Quality}
  \end{center}
  \end{table}
    \begin{table}[H]
  \begin{center}
  \begin{tabular}{ |c|c c c c c c c c c c|c| } 
  \hline
   & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & R\\ 
  \hline
	 0 & 5696 & 0 & 23 & 9 & 8 & 49 & 33 & 3 & 17 & 2 & 83 \\
	 1 & 1 & 6494 & 35 & 16 & 11 & 17 & 4 & 16 & 68 & 5 & 75 \\
	 2 & 21 & 17 & 5447 & 54 & 53 & 18 & 54 & 33 & 69 & 13 & 179 \\
	 3 & 5 & 16 & 82 & 5495 & 0 & 166 & 15 & 28 & 69 & 29 & 226 \\
	 4 & 6 & 17 & 38 & 2 & 5461 & 7 & 27 & 20 & 19 & 148 & 97 \\
	 5 & 24 & 7 & 29 & 329 & 15 & 4157 & 48 & 7 & 92 & 23 & 690 \\
	 6 & 27 & 11 & 62 & 2 & 27 & 79 & 5596 & 0 & 25 & 0 & 89 \\
	 7 & 9 & 19 & 46 & 24 & 33 & 11 & 3 & 5853 & 11 & 116 & 140 \\
	 8 & 11 & 73 & 51 & 108 & 13 & 101 & 28 & 11 & 5186 & 32 & 237 \\
	 9 & 13 & 18 & 22 & 41 & 146 & 21 & 2 & 169 & 35 & 5328 & 154 \\
   \hline
  \end{tabular}
  \end{center}
  \caption{Training Set Final Confusion Matrix}
  \end{table}
   \begin{table}[H]
  \begin{center}
  \begin{tabular}{ |c|c c c c c c c c c c|c| } 
  \hline
   & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & R\\ 
  \hline
	 0 & 948 & 0 & 3 & 2 & 0 & 5 & 10 & 1 & 1 & 0 & 10 \\
	 1 & 0 & 1099 & 2 & 2 & 0 & 4 & 1 & 2 & 13 & 0 & 12 \\
	 2 & 4 & 1 & 951 & 4 & 9 & 2 & 8 & 5 & 17 & 1 & 30 \\
	 3 & 0 & 0 & 8 & 929 & 0 & 15 & 0 & 7 & 13 & 3 & 35 \\
	 4 & 1 & 0 & 6 & 1 & 920 & 0 & 6 & 2 & 2 & 23 & 21 \\
	 5 & 5 & 2 & 5 & 56 & 2 & 688 & 6 & 1 & 20 & 5 & 102 \\
	 6 & 8 & 2 & 9 & 2 & 7 & 16 & 902 & 0 & 1 & 0 & 11 \\
	 7 & 1 & 4 & 22 & 8 & 3 & 0 & 0 & 944 & 3 & 17 & 26 \\
	 8 & 3 & 2 & 4 & 22 & 4 & 21 & 7 & 3 & 862 & 3 & 43 \\
	 9 & 5 & 5 & 2 & 8 & 28 & 1 & 0 & 27 & 5 & 900 & 28 \\
   \hline
  \end{tabular}
  \end{center}
  \caption{Test Set Final Confusion Matrix}
  \end{table}
\end{document}
